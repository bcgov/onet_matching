---
title: "The impact of Skill mis-match on labour market outcomes"
subtitle: "Source code: [![github logo](github.png){width=100px}](https://github.com/bcgov/onet_matching){target='_blank'}"
author: "Richard Martin"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 2
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message=FALSE)
set.seed(123)
library(tidyverse)
library(conflicted)
conflicts_prefer(dplyr::filter)
library(here)
library(lmtest)
library(sandwich)
library(tidytext)
library(broom)
library(ggalluvial)
library(margins)
my_dt <- function(tbbl) {
  DT::datatable(tbbl,
    extensions = "Buttons",
    rownames = FALSE,
    options = list(
      columnDefs = list(list(className = "dt-center", targets = "_all")),
      paging = TRUE,
      scrollX = TRUE,
      scrollY = TRUE,
      searching = TRUE,
      ordering = TRUE,
      dom = "Btip",
      buttons = list(
        list(extend = "csv", filename = "some_file_name"),
        list(extend = "excel", filename = "some_file_name")
      ),
      pageLength = 20,
      lengthMenu = c(3, 5)
    )
  )
}
```

```{r, cache=TRUE}
#simulation results----------------------
simulation_results <- read_rds(here("out","simulation results.rds"))
compare_swap <- read_rds(here("out","compare_swap.rds"))
noc_change <- read_rds(here("out","noc_change.rds"))
#for the heatmap------------------
distance2 <- read_csv(here("out","distance2.csv"))
dist_mat <- distance2|>
  mutate(NOC=str_sub(NOC, start=6),
         CIP=str_sub(CIP, start=6)
         )|>
  separate(NOC, into=c("NOC","NOC_number"), sep=": ")|>
  separate(CIP, into=c("CIP","CIP_number"), sep=": ")|>
  select(-NOC_number,-CIP_number)|>
  mutate(CIP=str_trunc(CIP, 50),
         NOC=str_trunc(NOC, 50))|>
  pivot_wider(names_from = CIP, values_from = distance)|>
  column_to_rownames("NOC")|>
  as.matrix()

#for proportion table-------------------
cip2_noc5 <- vroom::vroom(here("raw_data","stats_can","cip2_noc5.csv"), skip = 15, n_max = 512, col_names = FALSE)
colnames(cip2_noc5) <- vroom::vroom(here("raw_data","stats_can","cip2_noc5.csv"), skip = 10, n_max = 1, col_names = FALSE)
colnames(cip2_noc5)[1] <- "noc"
cip2_noc5 <- cip2_noc5|>
  mutate(noc=str_sub(noc, end=5))|>
  select(noc, `Health and related fields`)|>
  arrange(desc(`Health and related fields`))|>
  mutate(Proportion=`Health and related fields`/sum(`Health and related fields`),
         `Truncated Proportion`=if_else(Proportion>.01, Proportion, 0),
         `Adjusted Proportion`=`Truncated Proportion`/sum(`Truncated Proportion`)
         )|>
  slice_head(n=20)|>
  mutate(across(`Proportion`:`Adjusted Proportion`, ~ round(.x, 4)))
#for the income regression-------------------------
filtered <- read_rds(here("out","filtered.rds"))
filtering_info <- read_rds(here("out","filtering_info.rds"))
#for the logistic regression--------------------------
filtered2 <- read_rds(here("out","filtered2.rds"))
filtering_info2 <- read_rds(here("out","filtering_info2.rds"))

#for text---------------------
num_occupations <- length(unique(filtered$`NOC vs. Admin and financ...:`))
num_cip <- length(unique(filtered$`CIP vs. Agriculture...:`))
#split the data
train_and_test <- filtered|>
  mutate(set=sample(c("train", "test"), size=nrow(filtered), replace = TRUE, prob = c(.9, .1)))
train <- train_and_test|>
  filter(set=="train")|>
  select(-set)
test <- train_and_test|>
  filter(set=="test")|>
  select(-set)

train_and_test2 <- filtered2|>
  mutate(set=sample(c("train", "test"), size=nrow(filtered2), replace = TRUE, prob = c(.9, .1)))
train2 <- train_and_test2|>
  filter(set=="train")|>
  select(-set)

null_prob <- table(train2$full_time_full_year)[2]/nrow(train2)
test2 <- train_and_test2|>
  filter(set=="test")|>
  select(-set)

#the income model----------------
mod1 <- lm(log_income ~ . , data = train)
#the logit model-------------------------------
mod2 <- glm(full_time_full_year ~ . , family=binomial(link='logit'), data=train2)

margins_mod2 <- margins(mod2)|> #cant interpret logit (log odds ratio): margins translates to probabilities
  summary()|>
  separate(factor, into=c("variable", "level"), sep=":")

#assess accuracy-----------------
rmse_in_sample <- sqrt(mean(mod1$residuals^2))
test_w_pred <- test|>
  mutate(prediction= predict(mod1, newdata = test))
rmse_out_of_sample <- test_w_pred|>
  mutate(error_squared=(log_income-prediction)^2)|>
  summarize(mean_squared_error=mean(error_squared))|>
  pull()|>
  sqrt()

#rmse_in_sample2 <- sqrt(mean(mod2$residuals^2))

test_w_pred2 <- test2|>
  mutate(prediction= predict(mod2, newdata = test2),
         prediction=  round(exp(prediction)/(1+exp(prediction))), #inverse logit, then round to 0 or 1
         null_prediction = sample(c(0,1), size=nrow(test2), replace=TRUE, prob=c(1-null_prob, null_prob))
         )

confusion <- with(test_w_pred2, table(full_time_full_year, prediction))
prediction_accuracy <- (confusion[1,1]+confusion[2,2])/sum(confusion)

null_confusion <- with(test_w_pred2, table(full_time_full_year, null_prediction))
null_prediction_accuracy <- (null_confusion[1,1]+null_confusion[2,2])/sum(null_confusion)

test_logit <- glance(with(test_w_pred2, chisq.test(full_time_full_year, prediction)))
null_test_logit <- glance(with(test_w_pred2, chisq.test(full_time_full_year, null_prediction)))

# Adjust standard errors for stargazer
cov1         <- vcovHC(mod1, type = "HC1")
robust_se1    <- sqrt(diag(cov1))
cov2        <- vcovHC(mod2, type = "HC1")
robust_se2    <- sqrt(diag(cov2))

# Robust standard errors for plot ---------------
mod1rse <- coeftest(mod1, cov1)
mod1_coef <- broom::tidy(mod1rse, conf.int = TRUE)|>
  separate(term, into=c("variable","level"),sep=":`")|>
  mutate(variable=str_remove_all(variable, "`"),
         level= if_else(is.na(level), variable, level),
         level=str_trunc(level, 50)
         )|>
  filter(!variable %in% c("(Intercept)",
                          "Language vs. not english",
                          "Gender vs. Woman+"
                          ))|>
  arrange(variable, level)|>
  mutate(estimate=exp(estimate)-1,
         conf.low=exp(conf.low)-1,
         conf.high=exp(conf.high)-1
         )

mod2rse <- coeftest(mod2, cov2)
mod2_coef <- broom::tidy(mod2rse, conf.int = TRUE)|>
  separate(term, into=c("variable","level"),sep=":`")|>
  mutate(variable=str_remove_all(variable, "`"),
         level= if_else(is.na(level), variable, level),
         level=str_trunc(level, 50)
         )|>
  filter(!variable %in% c("(Intercept)",
                          "Language vs. not english",
                          "Gender vs. Woman+"
                          ))|>
  arrange(variable, level)|>
  mutate(estimate=exp(estimate)-1,
         conf.low=exp(conf.low)-1,
         conf.high=exp(conf.high)-1
         )

#for text--------------------------
distance_effect <- mod1_coef$estimate[mod1_coef$variable=="distance"]
dist_quant <- quantile(filtered$distance, probs = c(.1,.9), na.rm = TRUE)
economic_effect <- (dist_quant[2]-dist_quant[1])*distance_effect

distance_effect2 <- margins_mod2$AME[margins_mod2$variable=="distance"]
dist_quant2 <- quantile(filtered2$distance, probs = c(.1,.9), na.rm = TRUE)
economic_effect2 <- (dist_quant2[2]-dist_quant2[1])*distance_effect2
```

## TL;DR

People vary in how well matched their skills are to the skill required by their occupation. There are various reasons why people might be mis-matched.  For instance, if there is an element of "winner takes all" in the labour market, one would expect that the highest quality workers and highest quality employers would end up with the highest quality matches: both the buyer and the seller have the luxury of being selective in who they match with.  But being poorly matched does not *necessarily* indicate a worker is of low ability.  

* People might *choose* to be poorly matched if they discover they do not enjoy the work in the field they trained for.
* Poor match quality might be due to discrimination based on age, gender or ethnicity.  

Regardless of the cause of poor match quality, we will want to control for these confounding variables.  If we successfully control for *all* the factors that influence match quality, we can assume that match quality is "as good as" randomly assigned. The goal of this exercise is to investigate the impact of skill mis-match on 

1) **employment income of those who work full time full year:** Going from the 10th percentile of skill mis-match (relatively well matched) to the 90th percentile of skill mis-match (relatively poorly matched) causes a `r scales::percent(economic_effect, accuracy=.1)` reduction in employment income for those working full time, full year.

2) **the probability of working full time full year:** Going from the 10th percentile of skill mis-match (relatively well matched) to the 90th percentile of skill mis-match  (relatively poorly matched) causes a `r scales::percent(economic_effect2, accuracy=.1)` reduction in the probability of being employed full time full year. 

## Introduction

We define skill mis-match to be the euclidean distance between the skill profile of an occupation and the skills possessed by a worker.  The skill profile of an occupation is relatively straight forward to derive from the ONET skills and work activities.  In contrast there is no direct source of information regarding the skills of a worker. Here we make the presumption that a worker's skills are acquired during education, and we can infer the skills acquired during education by looking at the relationship between occupations and field of study.

Statistics Canada Table: 98-10-0403-01 contains counts of workers by occupation and field of study. Suppose that we wanted to infer the average skill profile of someone whose field of study was `Health and related fields`. In the table below we show the top occupations (by count of workers) for the field of study `Health and related fields`.  From this we derive a proportion, a truncated proportion (only proportions greater than .01), and an adjusted proportion to ensure the proportions we utilize sum to one.  We then multiply the skill profile of each of the occupations by the adjusted proportion and then sum to create a weighted average skill profile for each field of study.  At the extreme, if every person from a given field of study ended up in the same occupation, both skill profiles would be identical. 

```{r}
my_dt(cip2_noc5)
```

We undertake a similar exercise to create a weighted average skill profile of the `r num_occupations` aggregate occupations based on the 5 digit NOC skill profiles and weights based on the counts of workers.  

Once we have the skill profiles associated with each of the fields of study and occupation we can measure the distance between. To begin with we scale the skills and work activities to have a mean of zero and a standard deviation of one.  Next we perform principal component analysis, and retain only the first five principal components.  Finally we compute the euclidean distance between each of the occupations and fields of study based on the first five principal components.  The distances are ploted below, where colour indicates distance.  The most striking pattern is that for "Assisting occupations" the distance is large across all fields of study i.e. the yellow stripe in the bottom row.  

```{r, fig.width=10, fig.height=8}
 heatmaply::heatmaply(dist_mat)
```

## Filtering the census data:

The census public use micro data file contains a sample of roughly 1 million Canadians.  We perform the following filtering of the data prior to analysis of the impact of skill mis-match of the employment income of those who work full year full time.

```{r}
colnames(filtering_info) <- c("Filter applied","Observations left")
my_dt(filtering_info)
```

Note that the proportion of Canadians working full year full time is likely lower than normal, given COVID. This filtering leaves us with `r nrow(filtered)` observations, 90% of which are randomly allocated to a training set, with the remaining 10% going to a testing set. 

## Where do fields of study lead?

Using our filtered dataset, we can look at what fields of study are associated with what occupations. Note that it is the dispersion of occupations associated with a given field of study that leads to our measure of skill mis-match. e.g. if there was a field of study where every single person ends up in the same occupation, their skill mis-match (distance) would be zero.  In contrast, the larger the set of destination occupations, the more likely it is that the skills developed during education do not match exactly the skills required in one of the resulting occupations. 

```{r, fig.retina=2, fig.width=18, fig.height=18}
filtered|>
  group_by(`NOC vs. Admin and financ...:`, `CIP vs. Agriculture...:`)|>
  count()|>
  ggplot(aes(axis1= `CIP vs. Agriculture...:`, axis2 = `NOC vs. Admin and financ...:`, y = n)) +
  geom_alluvium(aes(fill = `CIP vs. Agriculture...:`)) +
  geom_stratum() +
  ggfittext::geom_fit_text(stat = "stratum", aes(label = after_stat(stratum)), width = 1/3, min.size = 1) +
  labs(fill="Fields of Study")+
  theme_void()+
  theme(legend.position = "bottom")
```




## The model:

So what we are after is the causal impact of a mis-match in skills (proxied by distance) on employment income.  Given that we are using observational data, we must lean on the Conditional Independence Assumption: i.e. we must assume that distance is "as good as randomly assigned" when we condition on the *all* the variables that influence both distance and employment income.  We fit the model 

$$\log(Employment~Income)=Age+Highest~Degree+ Language+ Gender+ Ethnicity+Occupation+Field~of~Study+distance+\mu$$
The plot below shows (most of the) regression results: Language and Gender estimates can be found in the regression table in the appendix.  From the results you can see that:

1) Employment income increases rapidly with age and then levels off.
2) Techy fields of study have higher levels of employment income, whereas humanities and education are low.
3) Employment income increases significantly with highest degree attained.
4) Employment income tends to be lower for non-whites.
5) Employment income highest for health occupations, lowest in sales.

But the main result we are after is the monetary penalty associated with a skill mis-match: $\beta_{distance}~=~$`r round(mod1_coef$estimate[mod1_coef$variable=="distance"],3)`, which can be interpreted as "A one unit increase in distance causes a $100*(\exp(\beta_{distance})-1)~=~$`r scales::percent(distance_effect, accuracy=.1)` change in employment income."  In terms of its economic relevance, going from the 10th percentile of distance `r dist_quant[1]` (relatively well matched) to the 90th percentile of distance `r dist_quant[2]` (relatively poorly matched) causes a `r scales::percent(economic_effect, accuracy=.1)` change in employment income, *ceteris paribus*. 

## Plot of regression results:

```{r, fig.retina=2, fig.width=12, fig.height=9}
ggplot(mod1_coef, aes(estimate,
                     reorder_within(level, within=variable, by=estimate),
                     xmin=conf.low,
                     xmax=conf.high))+
  geom_vline(xintercept = 0, col="grey", lty=2)+
  geom_point(size=.5)+
  geom_errorbarh(height=0)+
  facet_wrap(~variable, scales = "free")+
  scale_y_reordered()+
  scale_x_continuous(labels=scales::percent)+
  labs(x=NULL,
       y=NULL)+
  theme_minimal()
```

## Out of sample fit:

Note that the model only explains about `r scales::percent(glance(mod1)$r.squared)` of the variation in the *in sample* employment income, but even this might be overly optimistic. It is always a good idea to investigate the model's performance out of sample, to make sure that we have not over-fit the model.  Over fitting occurs when the model fits the in sample data well, but does not perform well out of sample. Below we look at how well the model performed out of sample (using the 10% of the sample that we held back).  The model does a fairly decent job of predicting employment income when it is low, but does not do a good job of explaining employment income in excess of \$300,000. Note that the root mean squared error in sample is `r round(rmse_in_sample,3)` whereas it is `r round(rmse_out_of_sample, 3)` out of sample: i.e. there is no evidence of over-fitting.


```{r}
ggplot(test_w_pred, aes(exp(prediction), exp(log_income)))+
  geom_abline(slope = 1, intercept = 0, col="white", lwd=2)+
  geom_point(alpha=.1)+
  scale_x_continuous(trans="log10", labels=scales::dollar)+
  scale_y_continuous(trans="log10", labels = scales::dollar)+
  labs(x="Prediction",
       y="Actual",
       title="Test set prediction errors")
```

## What if a social planner could reallocate workers between occupations?

Above we identified how much employment income would be expected to increase for an *individual* who went from being relatively well matched (10th percentile of distance) to relatively poorly matched (90th percentile of distance): a `r scales::percent(economic_effect, accuracy=.1)` change in employment income, *ceteris paribus*.  However this does not give an indication of the social welfare benefits of reducing mis-match, as it is possible that increasing the match quality of a given individual may worsen the match quality of whoever they displaced when they shifted occupations.  Next we perform a hypothetical exercise, where a social planner attempts to improve the average match quality by iteratively selecting two people at random, swapping their occupations, and comparing the original mis-match (distance) with the swapped mis-match (distance).  If the swapped distance is significantly lower than the original distance the workers swap occupations, otherwise they remain unchanged... and then we repeat 100000 times.  We then compare the predicted employment income based on observed occupations (factual) with the predicted employment income based on the swapped occupations (counter factual).  

[The simulation code &darr;]{style="float:right"}

```{r, eval=FALSE}
set.seed(123)
num_sim <- 100000 #social planner gives up after this many tries
cutoff <- .5 #only swap occupations if proportional improvement is greater than this
results <- tibble(sim = 1:num_sim, #initialize a dataframe to store simulation results
                  nocs = vector(length = num_sim, mode = "list"),
                  mean_distance = NA_real_)

for(i in 1:nrow(results)){
  if(i==1){#in the first iteration
    original <- test_stripped|> # test stripped contains only id, NOC and CIP
      left_join(cip_noc_diff,
                by = c("NOC21"= "census_noc_code", "CIP2021"="census_cip_code"))|> #adds in the distances
      mutate(prob=distance/sum(distance))#used below for drawing two observations to swap occupations
  }else{
    original <- test_stripped|>
      select(-NOC21)|> #get rid of the original allocation of NOCs
      bind_cols(NOC21=results$nocs[[i-1]])|> #add in the NOCs from the previous iteration
      left_join(cip_noc_diff,
                by = c("NOC21"= "census_noc_code",
                       "CIP2021"="census_cip_code"))|>#add distances (using last iterations NOCs)
      mutate(prob=distance/sum(distance)) #used below for drawing two observations to swap occupations
  }
    changepoints <- sample(original$id,
                           size=2,
                           replace = FALSE,
                           prob = original$prob) #two observations where we will swap NOCs
    new <- original|> #take  the original (for this iteration) data, then
      mutate(NOC21= replace(NOC21,
                            changepoints,
                            NOC21[rev(changepoints)]))|> #swap the NOCs for two of the observations
      select(-distance)|> #distance is now wrong given the swap
      left_join(cip_noc_diff,
                by = c("NOC21"= "census_noc_code",
                       "CIP2021"="census_cip_code"))#add in the correct distances
    proportion_improvement <- (sum(original$distance)-sum(new$distance))/mean(original$distance)
    if(proportion_improvement>cutoff){#if significant reduction in distance
      results$nocs[[i]] <- new$NOC21 #save the swapped NOCs for use in next iteration
      results$mean_distance[[i]] <- mean(new$distance) #save the new average distance
    }else{#if the swap did not reduce the mean distance
      results$nocs[[i]] <- original$NOC21 #save the unchanged NOCs for use in next iteration
      results$mean_distance[[i]] <- mean(original$distance) #save the original distance
    }
  print(paste(scales::percent(i/num_sim, accuracy = 1), "complete")) #how much longer do I have to wait?
}
```

```{r}
ggplot(simulation_results, aes(sim, mean_distance))+
  geom_line()+
  scale_x_continuous(labels=scales::comma)+
  labs(x="Simulation Number",
       y="Average skill gap distance",
       title="Shuffling occupations can reduce the average skill gap."
       )
```

What proportion of people had their occupations swapped?

```{r, comment=NA}
table(noc_change$unchanged)/nrow(noc_change)
```

So obviously this is a ridiculously unrealistic intervention: if `r scales::percent(as.vector(table(noc_change$unchanged)/nrow(noc_change))[1], accuracy=1)` of the labour market changed occupations it would be hugely disruptive. The point of the simulation is to figure out what the upper bound is on an intervention attempting to improve skill matching in the labour market: in the best case scenerio where there are no adjustment costs and it is cool to displace more than a third of the labour market, what improvement in the distribution of employment income can be attained? To answer this question we take the model that was fit on the training data, and make two predictions: one based actual occupations (and distances), and one based on the intervention (swapped occupations and distances).  First off, summary statistics:

```{r}
compare_swap|>
  group_by(data)|>
  summarise(mean_income=scales::dollar(mean(exp(predictions))),
            sd_income=scales::dollar(sd(exp(predictions))))|>
  DT::datatable(rownames = FALSE)
```

From the summary statistics we can infer that any risk neutral individual behind the veil of ignorance would prefer the distribution of employment income based on the swapped occupations: the mean is (slightly) higher.  How about for risk averse individuals?  Next, lets take a look at density plots:

```{r}
ggplot(compare_swap, aes(exp(predictions), fill=data))+
  geom_density(alpha=.25)+
  scale_x_continuous(labels = scales::dollar)+
  labs(x="Predicted Employment Income")
```

The reduction in dispersion is apparent, with the increase in level being more subtle.

Looks like it is possible that the swapped distribution of predicted employment income second order stochastically dominates (SOSD) the original distribution of predicted employment income.  Why do we care about SOSD?  What it implies is that **any** risk averse or risk neutral person behind the veil of ignorance would prefer the distribution of employment income based on the swapped occupations. 

```{r}
ggplot(compare_swap, aes(exp(predictions), colour = data)) +
  stat_ecdf()+
  scale_x_continuous(labels=scales::dollar)+
  annotate(geom = "text", x=60000, y=.22, label="a")+
  annotate(geom = "text", x=100000, y=.85, label="b")+
  labs(x="Predicted Employment Income",
       title="Second order stochastic dominance",
       subtitle="Given the area between the curves a is larger than area b, swapped SOSD original")
  
```

## The takeaway:

By costlessly displacing over a third of the labour market the social planner was able to marginally improve social outcomes, in the sense that *any* risk neutral or risk averse individual behind the veil of ignorance would prefer the government to intervene.  In reality, switching occupations is not without cost, so a more realistic intervention would likely focus on those *entering* the labour market. Thus the above exercise puts an upper bound on what we can hope to achieve by improving labour market skill matching.

## Does skill mis-match influence whether someone works full time full year?

In the analysis above we only considered those who were working full year full time in 2020: here we look at whether skills mis-match causes a difference in the odds of working full year full time.  We perform the following filtering of the data prior to analyzing the impact of skills mis-match on the odds of working full year full time.

```{r}
colnames(filtering_info2) <- c("Filter applied","Observations left")
my_dt(filtering_info2)
```


## The model:

So what we are after is the causal impact of a mis-match in skills (proxied by distance) on the probability of being employed full year full time.  Given that we are using observational data, we must lean on the Conditional Independence Assumption: i.e. we must assume that distance is "as good as randomly assigned" when we condition on the *all* the variables that influence both distance and whether employed full time full year.  We fit the model 

$$logit(Employed~FT~FY)=Age+Highest~Degree+ Language+ Gender+ Ethnicity+Occupation+Field~of~Study+distance+\mu$$
The plot below shows (most of the) regression results: Language and Gender estimates can be found in regression table which follows.  From the results you can see that the probability of working full year full time is:

1) significantly higher for every age group when compared to 20-24 year olds.
2) significantly higher for techy fields of study, and significantly lower for arts and humanities.
3) significantly higher for the highest degree attained, with the exception of Medicine, dentistry..
4) significantly higher for Filipino, and significantly lower for West Asian, Japanese and Korean.  
5) significantly higher for professional occupations, and significantly lower for a broad range of occupations.  

But the main result we are after is how distance affects the probability of being employed full time full year:  The probability of working full year full time decrease by `r scales::percent(distance_effect2, accuracy = .1)` for every one unit increase in distance.  In terms of its economic relevance, going from the 10th percentile of distance `r dist_quant2[1]` (relatively well matched) to the 90th percentile of distance `r dist_quant2[2]` (relatively poorly matched) causes a `r scales::percent(economic_effect2, accuracy=.1)` reduction in the probability of being fully employed, *ceteris paribus*.  

## Plot of regression results:

```{r, fig.retina=2, fig.width=12, fig.height=9}
margins_mod2|>
  mutate(variable=str_remove_all(variable, "`"),
         level= if_else(is.na(level), variable, level),
         level=str_trunc(level, 50)
         )|>
  filter(!variable %in% c("(Intercept)",
                          "Language vs. not english",
                          "Gender vs. Woman+"
                          ))|>
  arrange(variable, level)|>
  ggplot(aes(AME,
            reorder_within(level, within=variable, by=AME),
                     xmin=lower,
                     xmax=upper))+
  geom_vline(xintercept = 0, col="grey", lty=2)+
  geom_point(size=.5)+
  geom_errorbarh(height=0)+
  facet_wrap(~variable, scales = "free")+
  scale_y_reordered()+
  scale_x_continuous(labels=scales::percent)+
  labs(x=NULL,
       y=NULL)+
  theme_minimal()

# ggplot(mod2_coef, aes(estimate,
#                      reorder_within(level, within=variable, by=estimate),
#                      xmin=conf.low,
#                      xmax=conf.high))+
#   geom_vline(xintercept = 0, col="grey", lty=2)+
#   geom_point(size=.5)+
#   geom_errorbarh(height=0)+
#   facet_wrap(~variable, scales = "free")+
#   scale_y_reordered()+
#   scale_x_continuous(labels=scales::percent)+
#   labs(x=NULL,
#        y=NULL)+
#   theme_minimal()
```

## Assessing the Logit Model:

We use the logit model to form predictions based on the test data. These predictions are converted to probabilities of being employed full time full year, and then rounded to either zero or one. We then can look at the out of sample prediction accuracy of the model via a confusion matrix:

```{r, comment=NA}
confusion
```

From the confusion matrix we can see the model predicts full time full year employment with `r scales::percent(prediction_accuracy, accuracy=2)` accuracy. We can test the correspondence between the observed and predicted using the  `r test_logit$method`. i.e. What is the probability that we would get a correlation this strong between prediction and actual if the null hypothesis (no relationship) is true $$`r paste0("p = ",format(test_logit$p.value, scientific=FALSE))`$$ Lets compare this result to the null model, where we randomly assign either a zero or a one to each observation in the test set using the probabilities from the training set. 

```{r, comment=NA}
null_confusion
```

The NULL model predicts full time full year employment with `r scales::percent(null_prediction_accuracy, accuracy=2)` accuracy. Again, we can use the `r null_test_logit$method` to assess the probability that we would get a result this extreme if the null hypothesis is true `r paste0("p = ",round(null_test_logit$p.value,3))`.

## Appendix:

For those of you who like regression tables...  Note that logit results give odds ratio effects (not probabilities)

```{r, results='asis'}
stargazer::stargazer(mod1, mod2, type = "html",
          se = list(robust_se1, robust_se2))
```
  





